{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1D CNN Training Template\n\n",
        "This notebook mirrors the RNN workflow provided elsewhere in the project while swapping in a convolutional architecture defined in `utils.cnn_models`. Configure the `TrainingConfig` cell and execute the pipeline cells sequentially to train, validate, and optionally test a 1D CNN on the ECG heartbeat dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "from utils.cnn_models import ECG_CNN_Classifier\n",
        "from utils.data import calculate_class_weights, split_x_y\n",
        "from utils.preprocessing import Preprocessing\n",
        "from utils.torch_classes import ECG_Dataset, EarlyStopping\n",
        "from utils.train import test_loop, train_and_eval_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    '''Configuration options for the CNN training template.'''\n",
        "\n",
        "    num_classes: int = 5\n",
        "    batch_size: int = 256\n",
        "    learning_rate: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    dropout: float = 0.3\n",
        "    conv_channels: Tuple[int, ...] = (32, 64, 128)\n",
        "    kernel_sizes: Tuple[int, ...] = (7, 5, 3)\n",
        "    pool_kernel_sizes: Tuple[int, ...] = (2, 2, 2)\n",
        "    fc_hidden_dim: int = 128\n",
        "    use_batch_norm: bool = True\n",
        "\n",
        "    max_epochs: int = 30\n",
        "    patience: int = 6\n",
        "    delta: float = 1e-4\n",
        "    grad_clip: bool = True\n",
        "    max_norm: float = 5.0\n",
        "\n",
        "    scheduler_factor: float = 0.1\n",
        "    scheduler_patience: int = 3\n",
        "    min_lr: float = 1e-6\n",
        "\n",
        "    num_workers: int = 2\n",
        "    use_weighted_sampler: bool = True\n",
        "    debug: bool = True\n",
        "\n",
        "    checkpoint_path: str = \"models/best_CNN.pt\"\n",
        "    train_val_path: str = \"data/ecg_preprocessed_train_val.npz\"\n",
        "    test_csv_path: str = \"data/mitbih_test.csv\"\n",
        "\n",
        "    preprocess_test: bool = True\n",
        "    preprocessing_params: Dict[str, object] | None = None\n",
        "\n",
        "    device: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if self.preprocessing_params is None:\n",
        "            self.preprocessing_params = {\n",
        "                \"sample_freq\": 125,\n",
        "                \"cutoff_freq\": 25,\n",
        "                \"order\": 3,\n",
        "                \"target_r_peak_index\": 94,\n",
        "                \"method\": \"neurokit\",\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_sampler(y: np.ndarray) -> WeightedRandomSampler:\n",
        "    '''Create a ``WeightedRandomSampler`` based on class frequencies.'''\n",
        "\n",
        "    _, class_weights = calculate_class_weights(y)\n",
        "    sample_weights = np.array(class_weights, dtype=np.float64)[y]\n",
        "    weights_tensor = torch.as_tensor(sample_weights, dtype=torch.double)\n",
        "    return WeightedRandomSampler(\n",
        "        weights=weights_tensor,\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_dataloader(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    batch_size: int,\n",
        "    *,\n",
        "    sampler: Optional[WeightedRandomSampler] = None,\n",
        "    shuffle: bool = True,\n",
        "    num_workers: int = 0,\n",
        ") -> DataLoader:\n",
        "    '''Wrap numpy arrays inside a ``DataLoader``.'''\n",
        "\n",
        "    dataset = ECG_Dataset(X, y)\n",
        "    pin_memory = torch.cuda.is_available()\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,\n",
        "        shuffle=shuffle if sampler is None else False,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=num_workers > 0,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "\n",
        "def _to_numpy(array: torch.Tensor | np.ndarray) -> np.ndarray:\n",
        "    if isinstance(array, torch.Tensor):\n",
        "        return array.detach().cpu().numpy()\n",
        "    return array\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    y_true: torch.Tensor | np.ndarray,\n",
        "    y_pred: torch.Tensor | np.ndarray,\n",
        "    y_logits: torch.Tensor | np.ndarray,\n",
        "    average: str = \"macro\",\n",
        ") -> Dict[str, object]:\n",
        "    '''Compute a suite of evaluation metrics given model outputs.'''\n",
        "\n",
        "    y_true_np = _to_numpy(y_true)\n",
        "    y_pred_np = _to_numpy(y_pred)\n",
        "    y_logits_np = _to_numpy(y_logits)\n",
        "\n",
        "    logits_tensor = torch.from_numpy(y_logits_np).float()\n",
        "    probabilities = torch.softmax(logits_tensor, dim=1).numpy()\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true_np, y_pred_np),\n",
        "        \"precision_macro\": precision_score(\n",
        "            y_true_np, y_pred_np, average=average, zero_division=0\n",
        "        ),\n",
        "        \"recall_macro\": recall_score(\n",
        "            y_true_np, y_pred_np, average=average, zero_division=0\n",
        "        ),\n",
        "        \"f1_macro\": f1_score(y_true_np, y_pred_np, average=average, zero_division=0),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        metrics[\"roc_auc_ovr\"] = roc_auc_score(\n",
        "            y_true_np, probabilities, multi_class=\"ovr\", average=average\n",
        "        )\n",
        "    except ValueError:\n",
        "        metrics[\"roc_auc_ovr\"] = float(\"nan\")\n",
        "\n",
        "    metrics[\"classification_report\"] = classification_report(\n",
        "        y_true_np, y_pred_np, digits=4\n",
        "    )\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_single_run(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    config: TrainingConfig,\n",
        "):\n",
        "    '''Train a CNN model for a single train/validation split.'''\n",
        "\n",
        "    device = config.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = ECG_CNN_Classifier(\n",
        "        num_classes=config.num_classes,\n",
        "        conv_channels=config.conv_channels,\n",
        "        kernel_sizes=config.kernel_sizes,\n",
        "        pool_kernel_sizes=config.pool_kernel_sizes,\n",
        "        dropout=config.dropout,\n",
        "        fc_hidden_dim=config.fc_hidden_dim,\n",
        "        use_batch_norm=config.use_batch_norm,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        params=model.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=config.weight_decay,\n",
        "    )\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer=optimizer,\n",
        "        mode=\"min\",\n",
        "        factor=config.scheduler_factor,\n",
        "        patience=config.scheduler_patience,\n",
        "        min_lr=config.min_lr,\n",
        "    )\n",
        "\n",
        "    early_stopper = EarlyStopping(\n",
        "        patience=config.patience,\n",
        "        delta=config.delta,\n",
        "        checkpoint_path=config.checkpoint_path,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    sampler = build_sampler(y_train) if config.use_weighted_sampler else None\n",
        "\n",
        "    train_loader = create_dataloader(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=sampler,\n",
        "        shuffle=config.use_weighted_sampler is False,\n",
        "        num_workers=config.num_workers,\n",
        "    )\n",
        "\n",
        "    val_loader = create_dataloader(\n",
        "        X_val,\n",
        "        y_val,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "    )\n",
        "\n",
        "    history = train_and_eval_model(\n",
        "        model=model,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        train_dataloader=train_loader,\n",
        "        val_dataloader=val_loader,\n",
        "        epochs=config.max_epochs,\n",
        "        device=device,\n",
        "        early_stopper=early_stopper,\n",
        "        debug=config.debug,\n",
        "        verbose=True,\n",
        "        grad_clip=config.grad_clip,\n",
        "        max_norm=config.max_norm,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    best_epoch = int(np.argmin(history[\"val_loss\"]))\n",
        "    val_metrics = compute_metrics(\n",
        "        history[\"val_true\"][best_epoch],\n",
        "        history[\"val_pred\"][best_epoch],\n",
        "        history[\"val_pred_logits\"][best_epoch],\n",
        "    )\n",
        "\n",
        "    return model, {\"history\": history, \"val_metrics\": val_metrics, \"best_epoch\": best_epoch}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_on_test(\n",
        "    model: ECG_CNN_Classifier,\n",
        "    config: TrainingConfig,\n",
        "):\n",
        "    '''Load the best checkpoint (if any) and evaluate on the test split.'''\n",
        "\n",
        "    device = config.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    checkpoint_file = Path(config.checkpoint_path)\n",
        "    if checkpoint_file.exists():\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        print(\n",
        "            f\"Loaded checkpoint from {checkpoint_file} (epoch {checkpoint.get('epoch', 'N/A')}).\"\n",
        "        )\n",
        "\n",
        "    test_path = Path(config.test_csv_path)\n",
        "    if not test_path.exists():\n",
        "        print(\n",
        "            f\"Test file '{test_path}' not found. Skipping test evaluation.\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    X_test, y_test = split_x_y(test_df)\n",
        "\n",
        "    if config.preprocess_test:\n",
        "        preprocess = Preprocessing(**config.preprocessing_params)\n",
        "        X_test = preprocess.transform(X_test)\n",
        "\n",
        "    test_loader = create_dataloader(\n",
        "        X_test,\n",
        "        y_test,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "    )\n",
        "\n",
        "    results = test_loop(model=model, test_dataloader=test_loader, device=device)\n",
        "    metrics = compute_metrics(\n",
        "        results[\"y_true\"], results[\"y_pred\"], results[\"y_pred_logits\"],\n",
        "    )\n",
        "\n",
        "    return {\"metrics\": metrics, \"raw_outputs\": results}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training_pipeline(config: TrainingConfig) -> None:\n",
        "    '''Execute the full train/validation/test workflow.'''\n",
        "\n",
        "    train_val_path = Path(config.train_val_path)\n",
        "    if not train_val_path.exists():\n",
        "        print(\n",
        "            \"Preprocessed train/validation dataset not found at\",\n",
        "            f\" '{train_val_path}'. Please generate it before running the\",\n",
        "            \" CNN training template.\",\n",
        "        )\n",
        "        return\n",
        "\n",
        "    data = np.load(train_val_path)\n",
        "    X = data[\"X\"]\n",
        "    y = data[\"y\"]\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.05,\n",
        "        random_state=42,\n",
        "        stratify=y,\n",
        "    )\n",
        "\n",
        "    model, train_summary = train_single_run(\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_val=X_val,\n",
        "        y_val=y_val,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    print(\"\\nValidation metrics (best epoch):\")\n",
        "    for key, value in train_summary[\"val_metrics\"].items():\n",
        "        if key == \"classification_report\":\n",
        "            print(\"\\nClassification report:\\n\", value)\n",
        "        else:\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "    test_summary = evaluate_on_test(model=model, config=config)\n",
        "    if test_summary is not None:\n",
        "        print(\"\\nTest metrics:\")\n",
        "        for key, value in test_summary[\"metrics\"].items():\n",
        "            if key == \"classification_report\":\n",
        "                print(\"\\nClassification report:\\n\", value)\n",
        "            else:\n",
        "                print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage\n\n",
        "Instantiate a configuration, adjust any hyperparameters as needed, and execute the training pipeline:\n\n",
        "```python\n",
        "config = TrainingConfig()\n",
        "run_training_pipeline(config)\n",
        "```\n\n",
        "Running the final cell below will execute these steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = TrainingConfig()\n",
        "run_training_pipeline(config)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}